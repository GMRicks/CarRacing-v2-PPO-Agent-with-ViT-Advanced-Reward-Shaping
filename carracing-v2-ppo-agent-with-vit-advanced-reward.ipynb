{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing Important things\n","metadata":{}},{"cell_type":"code","source":"!apt-get update -qq\n!apt-get install -y swig\n!pip install Box2D-kengz --quiet\n!pip uninstall -y box2d-py\n!pip install gymnasium[box2d] --quiet\n!pip install imageio opencv-python timm --quiet\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:41:06.389987Z","iopub.execute_input":"2025-03-01T15:41:06.390389Z","iopub.status.idle":"2025-03-01T15:41:23.045241Z","shell.execute_reply.started":"2025-03-01T15:41:06.390358Z","shell.execute_reply":"2025-03-01T15:41:23.044214Z"}},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nswig is already the newest version (4.0.2-1ubuntu1).\n0 upgraded, 0 newly installed, 0 to remove and 142 not upgraded.\nFound existing installation: box2d-py 2.3.5\nUninstalling box2d-py-2.3.5:\n  Successfully uninstalled box2d-py-2.3.5\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import gymnasium as gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport imageio\nimport cv2\nfrom IPython.display import HTML\nfrom collections import deque\nfrom torch.distributions import Normal\nimport timm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T15:41:23.046908Z","iopub.execute_input":"2025-03-01T15:41:23.047241Z","iopub.status.idle":"2025-03-01T15:41:23.052877Z","shell.execute_reply.started":"2025-03-01T15:41:23.047209Z","shell.execute_reply":"2025-03-01T15:41:23.052134Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Multiframe Wrapping ","metadata":{}},{"cell_type":"code","source":"class FrameStackAvgWrapper(gym.Wrapper):\n    def __init__(self, env, num_stack=4):\n        super().__init__(env)\n        self.num_stack = num_stack\n        self.frames = deque([], maxlen=num_stack)\n        orig_shape = self.env.observation_space.shape  # e.g., (96, 96, 3)\n        self.H, self.W, self.C = orig_shape\n        self.observation_space = gym.spaces.Box(\n            low=0, high=255,\n            shape=(self.H, self.W, self.C * self.num_stack),\n            dtype=env.observation_space.dtype\n        )\n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        self.frames.clear()\n        for _ in range(self.num_stack):\n            self.frames.append(obs)\n        return self._get_obs(), info\n    def step(self, action):\n        obs, reward, done, truncated, info = self.env.step(action)\n        self.frames.append(obs)\n        return self._get_obs(), reward, done, truncated, info\n    def _get_obs(self):\n        return np.concatenate(list(self.frames), axis=-1)\n\nnum_stack = 4\nenv_id = \"CarRacing-v2\"\nenv = gym.make(env_id, render_mode=\"rgb_array\")\nenv = FrameStackAvgWrapper(env, num_stack=num_stack)\ntest_obs, _ = env.reset()\nprint(\"Stacked Observation Shape:\", test_obs.shape)  # Expect (96,96,12)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:43:34.154178Z","iopub.execute_input":"2025-03-01T16:43:34.154517Z","iopub.status.idle":"2025-03-01T16:43:34.193471Z","shell.execute_reply.started":"2025-03-01T16:43:34.154480Z","shell.execute_reply":"2025-03-01T16:43:34.192635Z"}},"outputs":[{"name":"stdout","text":"Stacked Observation Shape: (96, 96, 12)\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Vit Based Actor and Critic Network","metadata":{}},{"cell_type":"code","source":"class ViTFeatureExtractor(nn.Module):\n    def __init__(self, model_name=\"vit_base_patch16_224\"):\n        super(ViTFeatureExtractor, self).__init__()\n        self.vit = timm.create_model(model_name, pretrained=True)\n        for param in self.vit.parameters():\n            param.requires_grad = False\n        if hasattr(self.vit, 'fc'):\n            self.vit.fc = nn.Identity()\n        elif hasattr(self.vit, 'head'):\n            self.vit.head = nn.Identity()\n    def forward(self, x):\n        return self.vit(x)\n\nclass Actor(nn.Module):\n    def __init__(self, action_dim, feature_dim=768):\n        super(Actor, self).__init__()\n        self.feature_extractor = ViTFeatureExtractor()\n        self.fc = nn.Sequential(\n            nn.Linear(feature_dim, 256),\n            nn.ReLU()\n        )\n        self.mean_head = nn.Linear(256, action_dim)\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        x = self.fc(features)\n        mean = self.mean_head(x)\n        std = torch.exp(self.log_std)\n        return mean, std\n\nclass Critic(nn.Module):\n    def __init__(self, feature_dim=768):\n        super(Critic, self).__init__()\n        self.feature_extractor = ViTFeatureExtractor()\n        self.fc = nn.Sequential(\n            nn.Linear(feature_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        )\n    def forward(self, x):\n        features = self.feature_extractor(x)\n        return self.fc(features)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:45:10.398395Z","iopub.execute_input":"2025-03-01T16:45:10.398880Z","iopub.status.idle":"2025-03-01T16:45:10.406064Z","shell.execute_reply.started":"2025-03-01T16:45:10.398846Z","shell.execute_reply":"2025-03-01T16:45:10.405361Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"#  PPO Agent with ViT Feature Extraction","metadata":{}},{"cell_type":"code","source":"class PPOAgentViT:\n    def __init__(self, action_dim, lr=1e-4, gamma=0.99, lam=0.95, clip_epsilon=0.3, update_epochs=5, batch_size=32):\n        self.gamma = gamma\n        self.lam = lam\n        self.clip_epsilon = clip_epsilon\n        self.update_epochs = update_epochs\n        self.batch_size = batch_size\n\n        self.actor = Actor(action_dim).to(device)\n        self.critic = Critic().to(device)\n\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n\n    def process_obs(self, obs):\n        # obs shape: (96,96,12) -> split into 4 frames, each (96,96,3)\n        frames = np.split(obs, num_stack, axis=-1)\n        avg_frame = np.mean(frames, axis=0).astype(np.uint8)\n        resized = cv2.resize(avg_frame, (224, 224))\n        final_obs = np.transpose(resized, (2, 0, 1))  # (3,224,224)\n        return final_obs\n\n    def select_action(self, obs):\n        proc_obs = self.process_obs(obs)\n        obs_tensor = torch.FloatTensor(proc_obs).unsqueeze(0).to(device)\n        mean, std = self.actor(obs_tensor)\n        dist = Normal(mean, std)\n        sample = dist.sample()\n        action = torch.clamp(sample, -1.0, 1.0)\n        log_prob = dist.log_prob(sample).sum(dim=-1)\n        value = self.critic(obs_tensor)\n        return (\n            action.detach().cpu().numpy().flatten(),\n            log_prob.detach().cpu().numpy().flatten(),\n            value.detach().cpu().numpy().flatten()\n        )\n\n    def compute_gae(self, rewards, dones, values):\n        advantages = []\n        gae = 0\n        next_value = 0\n        for i in reversed(range(len(rewards))):\n            delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - values[i]\n            gae = delta + self.gamma * self.lam * gae * (1 - dones[i])\n            next_value = values[i]\n            advantages.insert(0, gae)\n        return advantages\n\n    def evaluate_actions(self, obs_batch, act_batch):\n        proc_obs = [self.process_obs(obs) for obs in obs_batch]\n        obs_tensor = torch.FloatTensor(np.array(proc_obs)).to(device)\n        if obs_tensor.dim() == 3:\n            obs_tensor = obs_tensor.unsqueeze(0)\n        act_tensor = torch.FloatTensor(np.array(act_batch)).to(device)\n        mean, std = self.actor(obs_tensor)\n        dist = Normal(mean, std)\n        log_probs = dist.log_prob(act_tensor).sum(dim=-1)\n        entropy = dist.entropy().sum(dim=-1)\n        values = self.critic(obs_tensor)\n        return log_probs, entropy, values\n\n    def update(self, trajectories):\n        obs_list = np.array([t[0] for t in trajectories])\n        act_list = np.array([t[1] for t in trajectories])\n        rew_list = np.array([t[2] for t in trajectories])\n        old_logp_list = np.array([t[3] for t in trajectories])\n        val_list = np.array([t[4] for t in trajectories])\n        done_list = np.array([t[5] for t in trajectories])\n\n        advantages = self.compute_gae(rew_list, done_list, val_list)\n        advantages = np.array(advantages)\n        returns = advantages + val_list\n\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        dataset_size = len(trajectories)\n        indices = np.arange(dataset_size)\n        for _ in range(self.update_epochs):\n            np.random.shuffle(indices)\n            start_idx = 0\n            while start_idx < dataset_size:\n                end_idx = min(start_idx + self.batch_size, dataset_size)\n                batch_idx = indices[start_idx:end_idx]\n                start_idx = end_idx\n\n                mb_obs = obs_list[batch_idx]\n                mb_act = act_list[batch_idx]\n                mb_old_logp = torch.FloatTensor(old_logp_list[batch_idx]).to(device)\n                mb_returns = torch.FloatTensor(returns[batch_idx]).to(device)\n                mb_adv = torch.FloatTensor(advantages[batch_idx]).to(device)\n\n                log_probs, entropy, values = self.evaluate_actions(mb_obs, mb_act)\n                ratio = torch.exp(log_probs - mb_old_logp)\n                surr1 = ratio * mb_adv\n                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * mb_adv\n                actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy.mean()\n                self.actor_optimizer.zero_grad()\n                actor_loss.backward()\n                self.actor_optimizer.step()\n\n                critic_loss = nn.MSELoss()(values.squeeze(1), mb_returns.squeeze(1))\n                self.critic_optimizer.zero_grad()\n                critic_loss.backward()\n                self.critic_optimizer.step()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:45:40.187203Z","iopub.execute_input":"2025-03-01T16:45:40.187493Z","iopub.status.idle":"2025-03-01T16:45:40.202931Z","shell.execute_reply.started":"2025-03-01T16:45:40.187470Z","shell.execute_reply":"2025-03-01T16:45:40.202067Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Advanced Reward Shaping","metadata":{}},{"cell_type":"code","source":"def advanced_reward_shaping(obs, original_reward, done, truncated):\n    r = original_reward + 0.1  # Living reward\n    # Process obs: average frames to a single image\n    proc = np.mean(obs.reshape(obs.shape[0], obs.shape[1], 3, -1), axis=-1)\n    center_region = proc[obs.shape[0]//2-10:obs.shape[0]//2+10, :]\n    avg_brightness = np.mean(center_region)\n    if 110 <= avg_brightness <= 140:\n        r += 0.3\n    else:\n        r -= 0.5\n    # Penalize excessive steering (assume first action element is steering)\n    # Since this is a demonstration, we subtract a penalty proportional to the absolute steering value.\n    # You may tune this coefficient further.\n    # Here, note: agent.select_action returns an action vector; we can pass that externally if needed.\n    # For simplicity, assume if original_reward is small, add extra penalty.\n    if original_reward < 0:\n        r -= 1.0\n    if done or truncated:\n        r -= 10.0\n    return r\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:46:02.054059Z","iopub.execute_input":"2025-03-01T16:46:02.054377Z","iopub.status.idle":"2025-03-01T16:46:02.059139Z","shell.execute_reply.started":"2025-03-01T16:46:02.054348Z","shell.execute_reply":"2025-03-01T16:46:02.058405Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"#  Training Loop & Visualization","metadata":{}},{"cell_type":"code","source":"agent = PPOAgentViT(action_dim=3, lr=1e-4, clip_epsilon=0.3, update_epochs=5, batch_size=32)\nnum_episodes = 500\nmax_timesteps = 1000\nall_rewards = []\n\nfor episode in range(num_episodes):\n    obs, _ = env.reset()\n    ep_reward = 0\n    trajectories = []\n    for t in range(max_timesteps):\n        action, logp, val = agent.select_action(obs)\n        next_obs, reward, done, truncated, _ = env.step(action)\n        shaped_r = advanced_reward_shaping(obs, reward, done, truncated)\n        trajectories.append((obs, action, shaped_r, logp, val, float(done or truncated)))\n        obs = next_obs\n        ep_reward += shaped_r\n        if done or truncated:\n            break\n    all_rewards.append(ep_reward)\n    agent.update(trajectories)\n    print(f\"Episode {episode}, Shaped Reward: {ep_reward}\")\n\nplt.figure(figsize=(10,6))\nplt.plot(all_rewards)\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Shaped Reward\")\nplt.title(\"Training Curve: CarRacing-v2 PPO with ViT Feature Extraction\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-01T16:57:30.079Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Real-Time Visual Demonstration","metadata":{}},{"cell_type":"code","source":"env_vis = gym.make(env_id, render_mode=render_mode)\nenv_vis = FrameStackAvgWrapper(env_vis, num_stack=num_stack)\ns, _ = env_vis.reset()\nframes = []\nfor t in range(1200):\n    frame = env_vis.render()\n    frames.append(frame)\n    a, _, _ = agent.select_action(s)\n    s_next, r, d, trunc, _ = env_vis.step(a)\n    s = s_next\n    if d or trunc:\n        break\nenv_vis.close()\nimageio.mimsave('carracing_vit_ppo.gif', frames, fps=30)\nHTML('<img src=\"carracing_vit_ppo.gif\">')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T16:49:59.775936Z","iopub.status.idle":"2025-03-01T16:49:59.776207Z","shell.execute_reply":"2025-03-01T16:49:59.776101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}